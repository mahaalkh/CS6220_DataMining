{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CS6220 - HW4 - Maha Alkhairy\n",
    "### Clustering and evaluation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of different types of clustering \n",
    "\"\"\"\n",
    "import math\n",
    "import random\n",
    "import sklearn \n",
    "import numpy as np \n",
    "import scipy.io \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import normalized_mutual_info_score, silhouette_score, calinski_harabaz_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written part: \n",
    "#### for more detail look at the scanned pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "a. The clusters given their points: \n",
    "* Cluster 1: {[0, 1], [1, 2], [2, 3], [3, 4]} \n",
    "* Cluster 2: {[5, 2], [6, 1], [7, 2], [6, 3]}\n",
    "\n",
    "b. Points which are density connected: \n",
    "* [0, 1], [1, 2], [2, 3] and [3, 4] are density connected\n",
    "* [5, 2], [6, 1], [7, 2] and [6, 3] are density connected \n",
    "\n",
    "c. Points considered as noise are: [0, 6], [0, 7] and [10, 3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ******  plotting the written part to make sense of data ****\n",
    "data_DBSCAN = np.array([[0, 1], \n",
    "                        [5, 2], \n",
    "                        [2, 3], \n",
    "                        [6, 1], \n",
    "                        [10, 2],\n",
    "                        [0, 6], \n",
    "                        [3, 4], \n",
    "                        [6, 3], \n",
    "                        [0, 7], \n",
    "                        [7, 2], \n",
    "                        [1, 2]])\n",
    "\n",
    "\n",
    "# print(data_DBSCAN)\n",
    "\n",
    "\n",
    "df_2 = DataFrame(data_DBSCAN, columns = ['x1', 'x2'])\n",
    "df_2.plot(kind='scatter', x = 'x1', y = 'x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "C<sub>1</sub> = {(5, 6), (8, 7), (7, 3)} <br/>\n",
    "C<sub>2</sub> = {(6, 5), (4, 5), (9, 2), (3, 5), (8, 4)}\n",
    "\n",
    "a. mean vectors: \n",
    " * m1 = [6.66666, 5.33333] (mean of first cluster (1 × 2))\n",
    " * m2 = [6, 4.2]            (mean of second cluster (1 × 2)) \n",
    " \n",
    "b. total mean vector: \n",
    "* m = [6.25, 4.625]       (mean of all points (1 × 2))  \n",
    "\n",
    "\n",
    "c. Scatter matrices of the clusters \n",
    "* S<sub>1</sub> = [[4.6666, 0.333333], [0.33333, 8.66666]]   (scatter matrix of cluster 1 (2 × 2)) \n",
    "* S<sub>2</sub> = [[26, -11], [-11, 6.8]]   (scatter matrix of cluster 2 (2 × 2))  \n",
    " \n",
    "d. Within cluster scatter matrix (2 × 2)\n",
    "* S<sub>w</sub> = S<sub>1</sub> + S<sub>2</sub> = [[30.6666, -10.6666], [-10.6666, 15.46666]]\n",
    "\n",
    "\n",
    "e. Between cluster scatter matrix (2 × 2)\n",
    "* S<sub>b</sub> = (m1 - m).T × (m1 - m) = [[0.83333, 1.4166666], [1.4166666, 2.40833333]]\n",
    "\n",
    "f. evaluation: based on the scattering criteria, this clustering is not good. \n",
    "* tr(S<sub>w</sub>)  = 30.6666 + 15.46666  = 46.1333 (want low) (add the diagonal elements in S<sub>W</sub>)\n",
    "* tr(S<sub>b</sub>)  = 0.83333 + 2.40833333 = 3.2416666 (want high) (add the diagonal elements in S<sub>B</sub>)\n",
    "* (tr(S<sub>b</sub>)  / tr(S<sub>w</sub>)) = 0.070267 (want high) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ******  plotting the written part to make sense of data ****\n",
    "cluster1 = [[5, 6], [8, 7], [7, 3]]\n",
    "cluster2 = [[6, 5], [4, 5], [9, 2], [3, 5], [8, 4]]\n",
    "\n",
    "# all_data = list(cluster1)\n",
    "# all_data.extend(cluster2)\n",
    "\n",
    "# cluster1_matrix = np.array(cluster1)\n",
    "# cluster2_matrix = np.array(cluster2)\n",
    "\n",
    "# all_data_matrix = np.array(all_data)\n",
    "\n",
    "\n",
    "df = DataFrame(np.array(cluster1), columns = ['x1', 'x2'])\n",
    "c1 = df.plot(kind='scatter', x = 'x1', y = 'x2')\n",
    "# plt.show()\n",
    "\n",
    "df1 = DataFrame(np.array(cluster2), columns = ['x1', 'x2'])\n",
    "df1.plot(kind='scatter', x = 'x1', y = 'x2', c = ['red'], ax = c1)\n",
    "plt.show()\n",
    "    \n",
    "# m1 = np.mean(cluster1_matrix, axis=0)\n",
    "# m2 = np.mean(cluster2_matrix, axis=0)\n",
    "# m = np.mean(all_data_matrix, axis=0)\n",
    "\n",
    "# C1_m1 = cluster1_matrix - m1\n",
    "# C2_m2 = cluster2_matrix - m2\n",
    "\n",
    "# S1 = np.dot(C1_m1.T, C1_m1)\n",
    "# S2 = np.dot(C2_m2.T, C2_m2)\n",
    "# S_W = S1 + S2 \n",
    "\n",
    "# m_1 = np.array([(m1 - m)])\n",
    "# m_2 = np.array([(m2 - m)])\n",
    "# S_B = np.dot(m_1.T, m_1) * 3 + np.dot(m_2.T, m_2) * 5\n",
    "# np.shape(S_B)\n",
    "# print(S_B.trace()/ S_W.trace())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: \n",
    "please look at scanned PDF : Maha_Alkhairy_HW4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 4, 5 and 6: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming part: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = np.loadtxt(\"datasets/dataset1.txt\",\n",
    "                    dtype = {'names': ('x1', 'x2', 'label'), 'formats': (float, float, int)},\n",
    "                    delimiter=\"\\t\")\n",
    "data2 = np.loadtxt(\"datasets/dataset2.txt\",\n",
    "                   dtype = {'names': ('x1', 'x2', 'label'), 'formats': (float, float, int)}, \n",
    "                   delimiter=\"\\t\")\n",
    "data3 = np.loadtxt(\"datasets/dataset3.txt\",\n",
    "                   dtype = {'names': ('x1', 'x2', 'label'), 'formats': (float, float, int)}, \n",
    "                   delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_io_matrix(d): \n",
    "    \"\"\"\n",
    "    formats the input data in a matrix format (n × d) where d is the number of dimentions and n is the number of data points \n",
    "    formats the label data in a matrix format (n × 1) where n is the number of data points \n",
    "    \"\"\"\n",
    "    matrix_input = [[x1, x2] for x1, x2, l in d]\n",
    "    matrix_labels = [l for x1, x2, l in d]\n",
    "    \n",
    "    return (np.array(matrix_input), np.array(matrix_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### number of clusters = 3\n",
    "data1_matrix, data1_labels = create_io_matrix(data1)\n",
    "# print(len(data1))\n",
    "print(np.shape(data1_matrix))\n",
    "# print(np.shape(data1_labels))\n",
    "# print(data1_labels)\n",
    "### number of cluster = 3\n",
    "data2_matrix, data2_labels = create_io_matrix(data2)\n",
    "# print(len(data2))\n",
    "# print(np.shape(data2_matrix))\n",
    "# print(np.shape(data2_labels))\n",
    "# print(data2_labels)\n",
    "### number of clusters = 2\n",
    "data3_matrix, data3_labels = create_io_matrix(data3)\n",
    "# print(len(data3))\n",
    "# print(np.shape(data3_matrix))\n",
    "# print(np.shape(data3_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############# initial data visualization \n",
    "\n",
    "df_1 = DataFrame(data1_matrix, columns = ['x1', 'x2'])\n",
    "df_1.plot(kind='scatter', x = 'x1', y = 'x2')\n",
    "\n",
    "print(\"=================== data 1 ======================\")\n",
    "plt.show()\n",
    "\n",
    "df_2 = DataFrame(data2_matrix, columns = ['x1', 'x2'])\n",
    "df_2.plot(kind='scatter', x = 'x1', y = 'x2')\n",
    "\n",
    "print(\"===================== data 2 ===================\")\n",
    "plt.show()\n",
    "\n",
    "df_3 = DataFrame(data3_matrix, columns = ['x1', 'x2'])\n",
    "df_3.plot(kind='scatter', x = 'x1', y = 'x2')\n",
    "\n",
    "print(\"================= data 3 ==================\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Algorithm (Q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### DBSCAN Algorithm: \n",
    "\n",
    "def DBSCAN(eps, min_points, input_matrix): \n",
    "    \"\"\"\n",
    "    implements the DBSCAN clustering algorithm \n",
    "    \"\"\"\n",
    "    visited = []\n",
    "    noise = []\n",
    "    clusters = {}\n",
    "    c_index = 0\n",
    "    \n",
    "    print(\"running DBSCAN with eps = {}, and min_pts = {}\".format(eps, min_points))\n",
    "  \n",
    "    for p in input_matrix:\n",
    "        if list(p) in visited: \n",
    "            continue\n",
    "            \n",
    "        if list(p) not in visited:\n",
    "            visited.append(list(p))\n",
    "            neighbors = region_query(p, eps, input_matrix) \n",
    "            if len(neighbors) < min_points:\n",
    "                noise.append(list(p))\n",
    "            else: \n",
    "                c_index += 1\n",
    "                visited, clusters, noise = expand_cluster(p, neighbors, eps, min_points,\n",
    "                                                          visited, clusters, c_index,\n",
    "                                                          input_matrix, noise)    \n",
    "    return (visited, clusters, noise)\n",
    "            \n",
    "\n",
    "def expand_cluster(p, neighbors, eps, min_points, visited, clusters, c_index, input_matrix, noise): \n",
    "    \"\"\"\n",
    "    gets the points in a cluster  \n",
    "    \"\"\"\n",
    "    clusters[c_index] = [list(p)]\n",
    "    for q in neighbors: \n",
    "        if list(q) not in visited: \n",
    "            visited.append(list(q))\n",
    "            new_neighbors = region_query(q, eps, input_matrix)\n",
    "            if len(new_neighbors) >= min_points: \n",
    "                neighbors.extend(new_neighbors)\n",
    "\n",
    "        if not member_cluster(list(q), clusters):\n",
    "            clusters[c_index].append(list(q))\n",
    "            if list(q) in noise: \n",
    "                noise.remove(list(q))\n",
    "            \n",
    "    return visited, clusters, noise\n",
    "            \n",
    "def member_cluster(q, clusters):\n",
    "    \"\"\"\n",
    "    checks if the point (q) has already been assigned a cluster \n",
    "    \"\"\"\n",
    "    for _, c in clusters.items(): \n",
    "        if list(q) in c:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def region_query(p, eps, input_matrix): \n",
    "    \"\"\"\n",
    "    :return: all points within P's eps-neighborhood (including P)\n",
    "    \"\"\"\n",
    "    eps_neighbors = [list(x) for x in input_matrix if (distance.euclidean(x, p) <= eps)]\n",
    "    return eps_neighbors\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### running DBSCAN on the data #1 \n",
    "\n",
    "print(\"running DBSCAN on the first dataset: \")\n",
    "visited111, clusters111, noise111 = DBSCAN(0.2, 2, data1_matrix)\n",
    "visited112, clusters112, noise112 = DBSCAN(0.2, 3, data1_matrix)\n",
    "visited113, clusters113, noise113 = DBSCAN(0.2, 4, data1_matrix)\n",
    "\n",
    "visited121, clusters121, noise121 = DBSCAN(0.3, 2, data1_matrix)\n",
    "visited122, clusters122, noise122 = DBSCAN(0.3, 3, data1_matrix)\n",
    "visited123, clusters123, noise123 = DBSCAN(0.3, 4, data1_matrix)\n",
    "\n",
    "visited131, clusters131, noise131 = DBSCAN(0.4, 2, data1_matrix)\n",
    "visited132, clusters132, noise132 = DBSCAN(0.4, 3, data1_matrix)\n",
    "visited133, clusters133, noise133 = DBSCAN(0.4, 4, data1_matrix)\n",
    "\n",
    "\n",
    "print(\"done running DBSCAN on the first dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### running DBSCAN on the data #2\n",
    "\n",
    "print(\"running DBSCAN on the second dataset: \")\n",
    "visited211, clusters211, noise211 = DBSCAN(0.8, 6, data2_matrix)\n",
    "visited212, clusters212, noise212 = DBSCAN(0.8, 7, data2_matrix)\n",
    "visited213, clusters213, noise213 = DBSCAN(0.8, 8, data2_matrix)\n",
    "\n",
    "visited221, clusters221, noise221 = DBSCAN(0.85, 6, data2_matrix)\n",
    "visited222, clusters222, noise222 = DBSCAN(0.85, 7, data2_matrix)\n",
    "visited223, clusters223, noise223 = DBSCAN(0.85, 8, data2_matrix)\n",
    "\n",
    "visited231, clusters231, noise231 = DBSCAN(0.9, 6, data2_matrix)\n",
    "visited232, clusters232, noise232 = DBSCAN(0.9, 7, data2_matrix)\n",
    "visited233, clusters233, noise233 = DBSCAN(0.9, 8, data2_matrix)\n",
    "\n",
    "\n",
    "print(\"done running DBSCAN on the second dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### running DBSCAN on the data #3\n",
    "\n",
    "print(\"running DBSCAN on the third dataset: \")\n",
    "\n",
    "visited311, clusters311, noise311 = DBSCAN(0.2, 5, data3_matrix)\n",
    "visited312, clusters312, noise312 = DBSCAN(0.2, 6, data3_matrix)\n",
    "visited313, clusters313, noise313 = DBSCAN(0.2, 7, data3_matrix)\n",
    "\n",
    "visited321, clusters321, noise321 = DBSCAN(0.3, 5, data3_matrix)\n",
    "visited322, clusters322, noise322 = DBSCAN(0.3, 6, data3_matrix)\n",
    "visited323, clusters323, noise323 = DBSCAN(0.3, 7, data3_matrix)\n",
    "\n",
    "visited331, clusters331, noise331 = DBSCAN(0.4, 5, data3_matrix)\n",
    "visited332, clusters332, noise332 = DBSCAN(0.4, 6, data3_matrix)\n",
    "visited333, clusters333, noise333 = DBSCAN(0.4, 7, data3_matrix)\n",
    "\n",
    "print(\"done running DBSCAN on the third dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating DBSCAN with different eps and min_pts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_clustering_labels_dbscan(clusters, noise, data_matrix, original_labels): \n",
    "    \"\"\"\n",
    "    gets the predicted labels for dbscan clusters\n",
    "    :return: data matrix which doesn't contain the noise points and\n",
    "    the original labels that exclude the labels of the noise points \n",
    "    \"\"\"\n",
    "    all_data = [list(p) for p in data_matrix]\n",
    "    changed_output = list(original_labels)\n",
    "    labels = []\n",
    "    for c_i, c in clusters.items(): \n",
    "        for p in c: \n",
    "            i = all_data.index(p)\n",
    "            labels.insert(i+1, c_i)\n",
    "    for p in noise: \n",
    "        i = all_data.index(p)\n",
    "        changed_output.pop(i)\n",
    "        all_data.remove(list(p)) \n",
    "        \n",
    "    return labels, np.asarray(all_data), changed_output\n",
    "\n",
    "def get_clustering_scores(data_matrix, true_labels, clusterings_and_noise): \n",
    "    \"\"\"\n",
    "    gets the NMI, SC, and CH scores of the clustering results \n",
    "    \"\"\"\n",
    "    clustering, noise, clustering_name = clusterings_and_noise\n",
    "    predicted_labels, changed_data, changed_true_labels = get_clustering_labels_dbscan(clustering, \n",
    "                                                                                       noise, \n",
    "                                                                                       data_matrix,\n",
    "                                                                                       true_labels)\n",
    "    print(clustering_name)\n",
    "    print(\"\\t NMI: \", normalized_mutual_info_score(changed_true_labels, predicted_labels))\n",
    "    print(\"\\t SC: \", silhouette_score(changed_data, predicted_labels)) \n",
    "    print(\"\\t CH: \", calinski_harabaz_score(changed_data, predicted_labels))\n",
    "    \n",
    "\n",
    "#### \n",
    "name_temp = \"clustering data {} using dbscan with eps = {} and  min_pts = {}\"\n",
    "\n",
    "clusterings_and_noise = {1: [\n",
    "                            (clusters111, noise111, name_temp.format(1, 0.2, 2)), \n",
    "                            (clusters112, noise112, name_temp.format(1, 0.2, 3)),\n",
    "                            (clusters113, noise113, name_temp.format(1, 0.2, 4)), \n",
    "\n",
    "                            (clusters121, noise121, name_temp.format(1, 0.3, 2)), \n",
    "                            (clusters122, noise122, name_temp.format(1, 0.3, 3)),\n",
    "                            (clusters123, noise123, name_temp.format(1, 0.3, 4)), \n",
    "\n",
    "                            (clusters131, noise131, name_temp.format(1, 0.4, 2)), \n",
    "                            (clusters132, noise132, name_temp.format(1, 0.4, 3)),\n",
    "                            (clusters133, noise133, name_temp.format(1, 0.4, 4))\n",
    "                            ], \n",
    "                        \n",
    "                        2: [\n",
    "                            (clusters211, noise211, name_temp.format(2, 0.8, 6)), \n",
    "                            (clusters212, noise212, name_temp.format(2, 0.8, 7)), \n",
    "                            (clusters213, noise213, name_temp.format(2, 0.8, 8)),\n",
    "\n",
    "                            (clusters221, noise221, name_temp.format(2, 0.85, 6)), \n",
    "                            (clusters222, noise222, name_temp.format(2, 0.85, 7)), \n",
    "                            (clusters223, noise223, name_temp.format(2, 0.85, 8)), \n",
    "\n",
    "                            (clusters231, noise231, name_temp.format(2, 0.9, 6)),\n",
    "                            (clusters232, noise232, name_temp.format(2, 0.9, 7)), \n",
    "                            (clusters233, noise233, name_temp.format(2, 0.9, 8))\n",
    "                           ], \n",
    "                        \n",
    "                        3: [\n",
    "                            (clusters311, noise311, name_temp.format(3, 0.2, 5)), \n",
    "                            (clusters312, noise312, name_temp.format(3, 0.2, 6)), \n",
    "                            (clusters313, noise313, name_temp.format(3, 0.2, 7)), \n",
    "\n",
    "                            (clusters321, noise321, name_temp.format(3,0.3, 5)), \n",
    "                            (clusters322, noise322, name_temp.format(3,0.3, 6)), \n",
    "                            (clusters323, noise323, name_temp.format(3,0.3, 7)), \n",
    "\n",
    "                            (clusters331, noise331 , name_temp.format(3, 0.4, 5)), \n",
    "                            (clusters332, noise332 , name_temp.format(3, 0.4, 6)), \n",
    "                            (clusters333, noise333 , name_temp.format(3, 0.4, 7))\n",
    "    ]} \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The scores of all the clusterings: \\n\")\n",
    "for i, cs_ns in clusterings_and_noise.items(): \n",
    "    for cs_n in cs_ns: \n",
    "        print(\" ---------------------------------------------- \")       \n",
    "        try: \n",
    "            if i == 1: \n",
    "                get_clustering_scores(data1_matrix, data1_labels, cs_n)\n",
    "            if i == 2: \n",
    "                get_clustering_scores(data2_matrix, data2_labels, cs_n)\n",
    "            if i == 3:\n",
    "                get_clustering_scores(data3_matrix, data3_labels, cs_n)\n",
    "        except ValueError as error: \n",
    "            print(\"only found one cluster \", cs_n[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotPointsBasedOnLabels(clusters, noise, colors, original_labels, data, shapes): \n",
    "    \"\"\"\n",
    "    black is noise\n",
    "    \"\"\"\n",
    "    K = len(clusters)\n",
    "    print(\"number of clusters: \", K)\n",
    "    all_points = [list(p) for p in data]\n",
    "    number_of_original_label = len(list(set(original_labels)))\n",
    "    for k in range(1, K+1): \n",
    "        for p in clusters[k]: \n",
    "            i = all_points.index(list(p))\n",
    "            plt.scatter(p[0], p[1], color=colors[k-1], marker=shapes[original_labels[i] - 1])\n",
    "    if noise:\n",
    "        for p in noise:\n",
    "            i = all_points.index(list(p))\n",
    "            plt.scatter(p[0], p[1], color=\"black\", marker=shapes[original_labels[i] - 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### only plotting the ones with the highest clustering evaluation score: \n",
    "\n",
    "print(\" clustering data 1 using dbscan with eps = 0.4 and  min_pts = 4\", \n",
    "      \"\\n \\t NMI: 1.0 \\t SC: 0.737772089744 \\t CH: 655.625678\")\n",
    "plotPointsBasedOnLabels(clusters133, noise133, [\"r\", \"g\", \"b\"], data1_labels, data1_matrix, ['o', '^', '*'])\n",
    "\n",
    "print(\"clustering data 2 using dbscan with eps = 0.9 and  min_pts = 6\", \n",
    "      \"\\n \\t NMI:  1.0 \\t SC:  0.639553233305 \\t CH:  406.62053703\") \n",
    "plotPointsBasedOnLabels(clusters231, noise231, [\"r\", \"g\", \"b\"], data2_labels, data2_matrix, ['o', '^', '*'])\n",
    "\n",
    "print(\"clustering data 3 using dbscan with eps = 0.3 and  min_pts = 7\", \n",
    "      \"\\n \\t NMI:  1.0 \\t SC:  0.362298541786 \\t CH:  162.813208069\")\n",
    "\n",
    "plotPointsBasedOnLabels(clusters323, noise323, [\"r\", \"g\", \"b\"], data3_labels, data3_matrix, ['o', '^', '*'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means implementation: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### K-means algorithm \n",
    "## * should preform multiple random initializations and keep the best \n",
    "## ---- not crash when initialization results in empty clusters\n",
    "\n",
    "\n",
    "def k_means(K, input_matrix, initial_means = False, num_restarts = 300):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    clusterings = []\n",
    "    sses = []\n",
    "    for i in range(num_restarts): \n",
    "        if i == 0: \n",
    "            cluster_assignments, cluster_means, sse = K_means_helper(K, input_matrix, initial_means = initial_means)\n",
    "            clusterings.append((cluster_assignments, cluster_means))\n",
    "            sses.append(sse)  \n",
    "        else: \n",
    "            cluster_assignments, cluster_means, sse = K_means_helper(K, input_matrix, initial_means = False)\n",
    "            clusterings.append((cluster_assignments, cluster_means))\n",
    "            sses.append(sse)  \n",
    "    return clusterings[sses.index(min(sses))][0],clusterings[sses.index(min(sses))][1] , min(sses)\n",
    "\n",
    "def K_means_helper(K, input_matrix, initial_means = False): \n",
    "    \"\"\"\n",
    "    K: number of clusters \n",
    "    ...\n",
    "    \"\"\"\n",
    "    N, d = np.shape(input_matrix)\n",
    "    if isinstance(initial_means, bool): \n",
    "        randomInts = [random.randint(1, N-1)  for i in range(K)]\n",
    "        cluster_means = [input_matrix[i] for i in randomInts]\n",
    "    else: \n",
    "        cluster_means = initial_means\n",
    "\n",
    "    cluster_assignments = get_cluster_assignments(K, cluster_means, input_matrix)\n",
    "    sse = compute_sse(input_matrix, cluster_assignments, cluster_means, K)\n",
    "\n",
    "    converged = False\n",
    "    i = 0 \n",
    "    while not converged:\n",
    "        prev_cluster_assignments = cluster_assignments\n",
    "        prev_means = cluster_means\n",
    "        prev_sse = sse\n",
    "\n",
    "\n",
    "        cluster_means = get_cluster_means(K, cluster_assignments, input_matrix)\n",
    "        cluster_assignments = get_cluster_assignments(K, cluster_means, input_matrix)\n",
    "        sse = compute_sse(input_matrix, cluster_assignments, cluster_means, K)\n",
    "\n",
    "        converged = converged_check(prev_cluster_assignments, cluster_assignments, cluster_means, prev_means, prev_sse, sse, i)\n",
    "        i = i + 1\n",
    "\n",
    "    return (cluster_assignments, cluster_means, sse)\n",
    "\n",
    "def converged_check(cluster_assignments_prev, cluster_assignments, cluster_means, cluster_means_prev, prev_sse, sse, i): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return  np.array_equal(cluster_assignments_prev, cluster_assignments) or\\\n",
    "            np.array_equal(cluster_means, cluster_means_prev) or\\\n",
    "            (prev_sse == sse) or\\\n",
    "            (i == 100)\n",
    "            \n",
    "            \n",
    "def get_cluster_assignments(K, cluster_means, input_matrix): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    N, d = np.shape(input_matrix)\n",
    "    cluster_assignments = np.zeros((N, K))\n",
    "    ### the rows are one hot vectors which indicate whether the data point is in the cluster\n",
    "    \n",
    "    for i in range(N): \n",
    "        x_i = input_matrix[i]\n",
    "        try: \n",
    "            distances = list(map(lambda m: distance.euclidean(x_i, m), cluster_means))\n",
    "            k = distances.index(min(distances))\n",
    "            cluster_assignments[i, k] = 1\n",
    "        except ValueError: \n",
    "#             print(\"error: mean = \", cluster_means)\n",
    "            continue\n",
    "      \n",
    "    return cluster_assignments\n",
    "        \n",
    "def get_cluster_means(K, cluster_assignments, input_matrix):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X = np.asarray(input_matrix)\n",
    "    z = np.asarray(cluster_assignments)\n",
    "    \n",
    "    ## K × 1\n",
    "    cc = np.sum(cluster_assignments, axis=0)\n",
    "\n",
    "    ## N × d --> N × 1 × d\n",
    "    ## N × K --> N × K × 1\n",
    "    ### N × K × d\n",
    "    \n",
    "    mult_data_prob = X[:, None, :] * z[:, :, None]\n",
    "    means = np.sum(mult_data_prob, axis = 0)\n",
    "    \n",
    "    if 0 in cc: \n",
    "#         print(\"have zero counts!! \", cc)\n",
    "        cc = cc + 1\n",
    "#         print(\"changed cc :( \", cc)\n",
    "        \n",
    "    new_means = means / cc[:, None]\n",
    "    \n",
    "    return new_means\n",
    "\n",
    "\n",
    "def compute_sse(input_matrix, clustering_assignments, cluster_means, K): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X = np.asarray(input_matrix)\n",
    "    z = np.asarray(clustering_assignments)\n",
    "    \n",
    "    ## K × d \n",
    "    mu = np.asarray(cluster_means)\n",
    "    \n",
    "    ## N \n",
    "    \n",
    "    ## N × K × d \n",
    "    X0 = X[:, None, :] * z[:, :, None]\n",
    "    X_m = X0[:, None, :] - mu[None, :, :]\n",
    "    \n",
    "    return ((X_m) ** 2).sum()\n",
    "    \n",
    "def getClusteredData(input_matrix, cluster_assignments, k):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    wanted = cluster_assignments[:, k]\n",
    "    wanted_X = []\n",
    "   \n",
    "    for i in range(len(wanted)):\n",
    "        if cluster_assignments[i][k] == 1: \n",
    "            wanted_X.append(input_matrix[i])\n",
    "            \n",
    "    return wanted_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_kmean_labels(input_matrix, cluster_assignments, centroids, original_labels, K, colors,  shapes):    \n",
    "    \"\"\"\n",
    "    black is noise\n",
    "    \"\"\"\n",
    "    all_points = [list(p) for p in input_matrix]\n",
    "    number_of_original_label = len(list(set(original_labels)))\n",
    "    for k in range(K):\n",
    "        clustered_k = getClusteredData(input_matrix, cluster_assignments, k)\n",
    "#         print(\"k: \", k, \"c_k: \", len(clustered_k))\n",
    "        for p in list(clustered_k): \n",
    "            i = all_points.index(list(p))\n",
    "            plt.scatter(p[0], p[1], color=colors[k-1], marker=shapes[original_labels[i] - 1])\n",
    "            \n",
    "    for c_m in centroids: \n",
    "#         print(c_m)\n",
    "        plt.scatter(c_m[0], c_m[1], color='black', s = 25, marker = 'x')\n",
    "   \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########### run k-means: \n",
    "\n",
    "def plot_k_means(data_matrix, K, original_labels, clustering_text, num_restarts=150, initial_means=False): \n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    print(clustering_text)\n",
    "    cluster_assignments, cluster_means, sse = k_means(K, data_matrix, num_restarts=num_restarts)\n",
    "    print(\"SSE: \", sse, \"\\n cluster means = \", cluster_means, \"\\n cluster counts: \", cluster_assignments.sum(0), \n",
    "          \"\\n total number of points\", cluster_assignments.sum())\n",
    "    plot_kmean_labels(data_matrix,\n",
    "                        cluster_assignments, \n",
    "                        cluster_means,\n",
    "                        original_labels, \n",
    "                        K, \n",
    "                        [\"red\", \"blue\", \"green\", \"pink\", \"yellow\"], \n",
    "                        [\"o\", \"^\", \"*\", \"s\", \"D\"])\n",
    "    return cluster_assignments, cluster_means, sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clustering_text = \"the clustering results of data {} with K = {}\"\n",
    "\n",
    "data1_cluster_assignments = [plot_k_means(data1_matrix, K, data1_labels, clustering_text.format(1, K)) for K in range(2, 6)]\n",
    "\n",
    "data2_cluster_assignments = [plot_k_means(data2_matrix, K, data2_labels, clustering_text.format(2, K)) for K in range(2, 6)]\n",
    "\n",
    "data3_cluster_assignments = [plot_k_means(data3_matrix, K, data3_labels, clustering_text.format(3, K)) for K in range(2, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of k-means with different Ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predicted_labels(clustering_assigments):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pred_labels = []\n",
    "    for c_a in clustering_assigments:\n",
    "        pred_labels.append(list(c_a).index(max(list(c_a))) + 1)\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clustering_scores_k_means(data_matrix, true_labels, clusterings_sse): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    cluster_assignments, cluster_means, sse = clusterings_sse\n",
    "    predicted_labels = get_predicted_labels(cluster_assignments)\n",
    "    \n",
    "    NMI = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "    SC = 0 \n",
    "    CH = 0\n",
    "    try: \n",
    "        SC = silhouette_score(data_matrix, predicted_labels)\n",
    "        CH = calinski_harabaz_score(data_matrix, predicted_labels)\n",
    "    except ValueError: \n",
    "        print(\"number of clusters = 1\")\n",
    "        \n",
    "    print(\"\\t NMI: \", NMI)\n",
    "    print(\"\\t SC: \", SC) \n",
    "    print(\"\\t CH: \", CH)\n",
    "    print(\"\\t SSE: \", sse)\n",
    "    print(\"------------------------------------------\")\n",
    "    \n",
    "    return (NMI, SC, CH, sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_clustering_data1 = [get_clustering_scores_k_means(data1_matrix, data1_labels, data1_cluster_assignments[i]) \n",
    "                           for i in range(4)]\n",
    "\n",
    "print(\"========================= scores for data 2 =========================\")\n",
    "\n",
    "scores_clustering_data2 = [get_clustering_scores_k_means(data2_matrix, data2_labels, data2_cluster_assignments[i]) \n",
    "                           for i in range(4)]\n",
    "\n",
    "print(\"========================= scores for data 3 =========================\")\n",
    "\n",
    "scores_clustering_data3 = [get_clustering_scores_k_means(data3_matrix, data3_labels, data3_cluster_assignments[i]) \n",
    "                           for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NMI_data1 = []\n",
    "SC_data1 = []\n",
    "CH_data1 = []\n",
    "sse_data1 = []\n",
    "\n",
    "for NMI, SC, CH, sse in scores_clustering_data1: \n",
    "    NMI_data1.append(NMI)\n",
    "    SC_data1.append(SC)\n",
    "    CH_data1.append(CH)\n",
    "    sse_data1.append(sse)\n",
    "###################################################    \n",
    "NMI_data2 = []\n",
    "SC_data2 = []\n",
    "CH_data2 = []\n",
    "sse_data2 = []\n",
    "\n",
    "for NMI, SC, CH, sse in scores_clustering_data2: \n",
    "    NMI_data2.append(NMI)\n",
    "    SC_data2.append(SC)\n",
    "    CH_data2.append(CH)\n",
    "    sse_data2.append(sse)\n",
    "\n",
    "########################################################\n",
    "\n",
    "NMI_data3 = []\n",
    "SC_data3 = []\n",
    "CH_data3 = []\n",
    "sse_data3 = []\n",
    "\n",
    "for NMI, SC, CH, sse in scores_clustering_data3: \n",
    "    NMI_data3.append(NMI)\n",
    "    SC_data3.append(SC)\n",
    "    CH_data3.append(CH)\n",
    "    sse_data3.append(sse)\n",
    "\n",
    "#######################################\n",
    "print(\"data 1 NMI scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], NMI_data1)\n",
    "plt.show()\n",
    "    \n",
    "print(\"data 1 CH scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], CH_data1)\n",
    "plt.show()\n",
    "\n",
    "print(\"data 1 SC scores for clusters with k in the x axis and the score in the y\")  \n",
    "plt.plot([2, 3, 4, 5], SC_data1)\n",
    "plt.show()\n",
    "\n",
    "print(\"data 1 sse scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], sse_data1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "print(\"data 2 NMI scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], NMI_data2)\n",
    "plt.show()\n",
    "    \n",
    "print(\"data 2 CH scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], CH_data2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"data 2 SC scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], SC_data2)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"data 2 sse scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], sse_data2)    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "#######################################\n",
    "print(\"data 3 NMI scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], NMI_data3) \n",
    "plt.show()\n",
    "    \n",
    "print(\"data 3 CH scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], CH_data3)\n",
    "plt.show()\n",
    "\n",
    "print(\"data 3 SC scores for clusters with k in the x axis and the score in the y\")  \n",
    "plt.plot([2, 3, 4, 5], SC_data3)\n",
    "plt.show()\n",
    "\n",
    "print(\"data 3 sse scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], sse_data3)\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM (Gaussian Mixure Model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Expectation Maximization for gaussian mixture models \n",
    "## * should perform multiple restarts and accept initial values for the mean and covariance as input \n",
    "\n",
    "## initialize the the mean by sampling from a gaussian distribution centered on a random point in the data\n",
    "\n",
    "## initialize the variance along each dimension to a random fraction of the total variance in the data\n",
    "\n",
    "### calculating the log probabilities rather than the probabilies themselves helps prevent\n",
    "## underflow since the log of a very small number  < 1 is a negative number which is > 1\n",
    "\n",
    "def GMM(data_matrix, K, num_restarts = 300, means = None, variances = None): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    assignments = []\n",
    "    log_likes = []\n",
    "    \n",
    "    for i in range(num_restarts): \n",
    "        if i == 0: \n",
    "            means, variances, soft_assignments, log_likelihood = GMM_helper(data_matrix,\n",
    "                                                                            K, \n",
    "                                                                            means = means,\n",
    "                                                                            variances = variances)\n",
    "            if math.isnan(log_likelihood):\n",
    "                continue\n",
    "\n",
    "            assignments.append((means, variances, soft_assignments))\n",
    "            log_likes.append(log_likelihood)\n",
    "\n",
    "        else:\n",
    "            means, variances, soft_assignments, log_likelihood = GMM_helper(data_matrix,\n",
    "                                                                            K, \n",
    "                                                                            means = None,\n",
    "                                                                            variances = None)\n",
    "            if math.isnan(log_likelihood):\n",
    "                continue\n",
    "            assignments.append((means, variances, soft_assignments))\n",
    "            log_likes.append(log_likelihood)\n",
    "\n",
    "        \n",
    "    return assignments[log_likes.index(max(log_likes))][0], assignments[log_likes.index(max(log_likes))][1] , assignments[log_likes.index(max(log_likes))][2], max(log_likes)\n",
    "\n",
    "\n",
    "def GMM_helper(data_matrix, K, means = None, variances = None): \n",
    "    \"\"\"\n",
    "    1) assuming the covariance matrix for each cluster is diagonal \n",
    "    A) initialize the the mean by sampling from a gaussian distribution centered on a random point in the data\n",
    "    B) Initialize the variance along each dimension to a random fraction of the total variance in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    N, d = np.shape(data_matrix)\n",
    "    means, variances, soft_assignments, cluster_counts, cluster_label_prob, log_likelihood =  get_initial_values(data_matrix,\n",
    "                                                                                                                 K, \n",
    "                                                                                                                 means = means, \n",
    "                                                                                                                variances = variances)\n",
    "    converged = False\n",
    "    \n",
    "    i = 0 \n",
    "    while not converged:\n",
    "        prev_soft_assignments = soft_assignments\n",
    "        prev_means, prev_covars = means, variances\n",
    "        log_likelihood_prev = log_likelihood\n",
    "  \n",
    "        means = get_means(data_matrix, soft_assignments, cluster_counts, K)\n",
    "        variances = get_vars(data_matrix, soft_assignments, means, cluster_counts, K)\n",
    "     \n",
    "        ## 1 × K counts \n",
    "        cluster_counts = get_cluster_counts(soft_assignments, N, K)\n",
    "        \n",
    "        ## 1 × K probabilities \n",
    "        cluster_label_prob = cluster_counts / N\n",
    "        \n",
    "        log_joint = get_log_joint(data_matrix, means, variances, cluster_label_prob)\n",
    "\n",
    "        soft_assignments = get_soft_assignments(log_joint)\n",
    "\n",
    "        log_likelihood = compute_log_likelihood(log_joint)\n",
    "\n",
    "        converged = converged_check(log_likelihood_prev, log_likelihood, i)\n",
    "        i = i + 1\n",
    "        \n",
    "    return means, variances, soft_assignments, log_likelihood\n",
    "    \n",
    "    \n",
    "def get_initial_values(data_matrix, K, means = None, variances = None): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    N, d = np.shape(data_matrix)\n",
    "    \n",
    "#     if (means is None) and (variances is None): \n",
    "    total_variance = np.var(data_matrix)\n",
    "\n",
    "    ## choose any number between 0 and 1 \n",
    "    fraction_var_factor = random.uniform(0, 1) \n",
    "    # print(\"factor = \", fraction_var_factor)\n",
    "    init_variance_dim = [total_variance * fraction_var_factor for i in range(d)]\n",
    "\n",
    "    fraction_factor = random.randint(2, 5) \n",
    "    std_fraction = (total_variance / (fraction_factor * K)) ** 0.5\n",
    "\n",
    "    randomInts = [random.randint(0, N-1) for i in range(K)]\n",
    "\n",
    "    means = np.asarray([np.random.normal(loc=data_matrix[i], scale=std_fraction, size=(1, d)) for i in randomInts]).sum(1)\n",
    "    variances = np.asarray([init_variance_dim] * K)        \n",
    "    \n",
    "    ## 1 × K counts \n",
    "    cluster_counts = get_cluster_counts([], N, K, init=True)\n",
    "    \n",
    "    [N/K] * K\n",
    "\n",
    "    ## 1 × K probabilities \n",
    "    cluster_label_prob = [N_k/N for N_k in cluster_counts]\n",
    "    \n",
    "    log_joint = get_log_joint(data_matrix, means, variances, cluster_label_prob)\n",
    "    \n",
    "    soft_assignments = get_soft_assignments(log_joint)\n",
    "    \n",
    "    log_likelihood = compute_log_likelihood(log_joint)\n",
    "    \n",
    "    \n",
    "    return means, variances, soft_assignments, cluster_counts, cluster_label_prob, log_likelihood\n",
    "    \n",
    "\n",
    "def get_means(data_matrix, soft_assignments, cluster_counts, K): \n",
    "    \"\"\"\n",
    "    gets the means of the clusters \n",
    "    \"\"\"\n",
    "    \n",
    "    N, d = np.shape(data_matrix)\n",
    "    \n",
    "    ## K × 1\n",
    "    cc = np.asarray(cluster_counts)\n",
    "        \n",
    "    X = np.asarray(data_matrix)\n",
    "    g = np.asarray(soft_assignments)\n",
    "    \n",
    "    ## N × d --> N × 1 × d\n",
    "    ## N × K --> N × K × 1\n",
    "    ### N × K × d\n",
    "    \n",
    "    mult_data_prob = X[:, None, :] * g[:, :, None]\n",
    "    means = np.sum(mult_data_prob, axis = 0)\n",
    "        \n",
    "    return means / cc[:, None]\n",
    "    \n",
    "\n",
    "def get_vars(data_matrix, soft_assignments, means, cluster_counts, K): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X = np.asarray(data_matrix)\n",
    "    mu = np.asarray(means)\n",
    "    g = np.asarray(soft_assignments)\n",
    "    \n",
    "    # N × d --> N × 1 × d\n",
    "    ## K × d --> 1 × K × d\n",
    "    ### N × K × d\n",
    "    \n",
    "    ## K × 1\n",
    "    cc = np.asarray(cluster_counts)\n",
    "    \n",
    "    x_m = X[:, None, :] - mu[None, :, :]\n",
    "    \n",
    "    sigmas = (g[:, :, None] * x_m ** 2).sum(0)\n",
    "        \n",
    "    return sigmas / cc[:, None]\n",
    "    \n",
    "\n",
    "def get_cluster_counts(soft_assignments, N, K, init=False): \n",
    "    \"\"\"\n",
    "    soft_assignments: N × K\n",
    "    \"\"\"\n",
    "    if init: \n",
    "        return [N/K] * K\n",
    "    else: \n",
    "        cluster_counts = soft_assignments.sum(axis = 0)\n",
    "        return np.array(cluster_counts)\n",
    "\n",
    "\n",
    "def get_log_joint(data_matrix, means, covars, cluster_label_prob): \n",
    "    \"\"\"\n",
    "    fix the log likelihood based on gaussian formula\n",
    "    \n",
    "    N × K \n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.asarray(data_matrix)\n",
    "    mu = np.asarray(means)\n",
    "    sigma = np.asarray(covars)       \n",
    "    pi = np.asarray(cluster_label_prob)\n",
    "    \n",
    "    X0 = (X[:,None,:] - mu[None,:,:]) ** 2 / sigma[None,:,:]\n",
    "    log_like = (- np.log(2 * np.pi) - 0.5 * np.log(sigma[None,:,:]) - 0.5 * X0).sum(axis=2)\n",
    "    log_prior = np.log(pi[None,:])\n",
    "    \n",
    "    return log_like + log_prior\n",
    "    \n",
    "def get_soft_assignments(log_joint): \n",
    "    \"\"\"\n",
    "    the cluster assignments is a N × K \n",
    "    matrix that specifies probability that each point belongs to cluster k\n",
    "    \"\"\"\n",
    "    w = np.asarray(log_joint)\n",
    "    w_max = np.max(w, axis = 1)\n",
    "    w_0 = w - w_max[:, None]\n",
    "    p = np.exp(w_0)\n",
    "    return p / p.sum(axis=1)[:, None]\n",
    "\n",
    "def compute_log_likelihood(log_joint, soft_assignments=None): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if soft_assignments is None:\n",
    "        soft_assignments = get_soft_assignments(log_joint)\n",
    "    \n",
    "    return (soft_assignments * (log_joint - np.log(soft_assignments))).sum()\n",
    "\n",
    "def converged_check(log_likelihood_prev, log_likelihood, i): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if log_likelihood_prev > log_likelihood:\n",
    "        pass\n",
    "#         print(\"log likelihood became smaller !!!\")\n",
    "        \n",
    "    \n",
    "    if i == 99: \n",
    "        pass \n",
    "#         print(\"i = 99\")\n",
    "        \n",
    "    return (i == 100) or (abs(log_likelihood_prev - log_likelihood) <= 0.01) or (log_likelihood_prev > log_likelihood)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predicted_labels_GMM(soft_assignments): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    g = np.asarray(soft_assignments)\n",
    "    return np.argmax(g, axis = 1) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### plot the clusters and do the plots for evals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_points_in_cluster(data_matrix, predicted_labels): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    X = np.asarray(data_matrix)\n",
    "    c_a = np.asarray(predicted_labels)\n",
    "    K = np.max(predicted_labels)\n",
    "    N, d = np.shape(X)\n",
    "    \n",
    "    clustered_points = [[X[i] for i in range(N) if (c_a[i] == k)] for k in range(1, K+1)]\n",
    "    \n",
    "    return clustered_points\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_GMM_labels(input_matrix, predicted_labels, means, original_labels, K, colors,  shapes):    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    all_points = [list(p) for p in input_matrix]\n",
    "    points_Ks = get_points_in_cluster(input_matrix, predicted_labels)\n",
    "    for k in range(K):\n",
    "        for p in list(points_Ks[k]): \n",
    "            i = all_points.index(list(p))\n",
    "            plt.scatter(p[0], p[1], color=colors[k-1], marker=shapes[original_labels[i] - 1])\n",
    "            \n",
    "    for c_m in means: \n",
    "        plt.scatter(c_m[0], c_m[1], color='black', s = 25, marker = 'x')\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_GMM(data_matrix, K, original_labels, clustering_text, num_restarts=100, initial_means=None, initial_vars=None): \n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    print(clustering_text)\n",
    "    means, variances, soft_assignments, log_likelihood = GMM(data_matrix, K, num_restarts = 300, means = initial_means, variances = initial_vars)\n",
    "    predicted_labels = get_predicted_labels_GMM(soft_assignments)\n",
    "    \n",
    "    plot_GMM_labels(data_matrix, \n",
    "                    predicted_labels,\n",
    "                    means,\n",
    "                    original_labels,\n",
    "                    K, \n",
    "                    [\"red\", \"blue\", \"green\", \"pink\", \"yellow\"], \n",
    "                    [\"o\", \"^\", \"*\", \"s\", \"D\"])\n",
    "    \n",
    "    return (means, variances, predicted_labels, log_likelihood)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustering_text = \"GMM: the clustering results of data {} with K = {}\"\n",
    "\n",
    "data1_cluster_assignments_GMM = [plot_GMM(data1_matrix, K, data1_labels, clustering_text.format(1, K)) for K in range(2, 6)]\n",
    "\n",
    "data2_cluster_assignments_GMM = [plot_GMM(data2_matrix, K, data2_labels, clustering_text.format(2, K)) for K in range(2, 6)]\n",
    "\n",
    "data3_cluster_assignments_GMM = [plot_GMM(data3_matrix, K, data3_labels, clustering_text.format(3, K)) for K in range(2, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clustering_scores_GMM(data_matrix, true_labels, GMM_results): \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    means, variances, predicted_labels, log_likelihood = GMM_results\n",
    "    NMI = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "    SC = 0 \n",
    "    CH = 0\n",
    "    try: \n",
    "        SC = silhouette_score(data_matrix, predicted_labels)\n",
    "        CH = calinski_harabaz_score(data_matrix, predicted_labels)\n",
    "    except ValueError: \n",
    "        print(\"number of clusters = 1\")\n",
    "        \n",
    "    print(\"\\t NMI: \", NMI)\n",
    "    print(\"\\t SC: \", SC) \n",
    "    print(\"\\t CH: \", CH)\n",
    "    print(\"\\t Log Likelihood: \", log_likelihood)\n",
    "    print(\"------------------------------------------\")\n",
    "    \n",
    "    return (NMI, SC, CH, log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_clustering_data1_GMM = [get_clustering_scores_GMM(data1_matrix, data1_labels, data1_cluster_assignments_GMM[i]) \n",
    "                           for i in range(4)]\n",
    "\n",
    "print(\"========================= scores for data 2 =========================\")\n",
    "\n",
    "scores_clustering_data2_GMM = [get_clustering_scores_GMM(data2_matrix, data2_labels, data2_cluster_assignments_GMM[i]) \n",
    "                           for i in range(4)]\n",
    "\n",
    "print(\"========================= scores for data 3 =========================\")\n",
    "\n",
    "scores_clustering_data3_GMM = [get_clustering_scores_GMM(data3_matrix, data3_labels, data3_cluster_assignments_GMM[i]) \n",
    "                           for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NMI_data1_GMM = []\n",
    "SC_data1_GMM = []\n",
    "CH_data1_GMM = []\n",
    "ll_data1_GMM = []\n",
    "\n",
    "for NMI, SC, CH, ll in scores_clustering_data1_GMM: \n",
    "    NMI_data1_GMM.append(NMI)\n",
    "    SC_data1_GMM.append(SC)\n",
    "    CH_data1_GMM.append(CH)\n",
    "    ll_data1_GMM.append(ll)\n",
    "###################################################    \n",
    "NMI_data2_GMM = []\n",
    "SC_data2_GMM = []\n",
    "CH_data2_GMM = []\n",
    "ll_data2_GMM = []\n",
    "\n",
    "for NMI, SC, CH, sse in scores_clustering_data2_GMM: \n",
    "    NMI_data2_GMM.append(NMI)\n",
    "    SC_data2_GMM.append(SC)\n",
    "    CH_data2_GMM.append(CH)\n",
    "    ll_data2_GMM.append(ll)\n",
    "\n",
    "########################################################\n",
    "\n",
    "NMI_data3_GMM = []\n",
    "SC_data3_GMM = []\n",
    "CH_data3_GMM = []\n",
    "ll_data3_GMM = []\n",
    "\n",
    "for NMI, SC, CH, ll in scores_clustering_data3_GMM: \n",
    "    NMI_data3_GMM.append(NMI)\n",
    "    SC_data3_GMM.append(SC)\n",
    "    CH_data3_GMM.append(CH)\n",
    "    ll_data3_GMM.append(ll)\n",
    "\n",
    "####################################################\n",
    "print(\"GMM: data 1 NMI scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], NMI_data1_GMM)\n",
    "plt.show()\n",
    "    \n",
    "print(\"GMM: data 1 CH scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], CH_data1_GMM)\n",
    "plt.show()\n",
    "\n",
    "print(\"GMM: data 1 SC scores for clusters with k in the x axis and the score in the y\")  \n",
    "plt.plot([2, 3, 4, 5], SC_data1_GMM)\n",
    "plt.show()\n",
    "\n",
    "print(\"GMM: data 1 log likelihood scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], ll_data1_GMM)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "print(\"data 2 NMI scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], NMI_data2_GMM)\n",
    "plt.show()\n",
    "    \n",
    "print(\"data 2 CH scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], CH_data2_GMM)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"data 2 SC scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], SC_data2_GMM)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"data 2 log likelihood scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], ll_data2_GMM)    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "#######################################\n",
    "print(\"data 3 NMI scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], NMI_data3_GMM) \n",
    "plt.show()\n",
    "    \n",
    "print(\"data 3 CH scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], CH_data3_GMM)\n",
    "plt.show()\n",
    "\n",
    "print(\"data 3 SC scores for clusters with k in the x axis and the score in the y\")  \n",
    "plt.plot([2, 3, 4, 5], SC_data3_GMM)\n",
    "plt.show()\n",
    "\n",
    "print(\"data 3 log likelihood scores for clusters with k in the x axis and the score in the y\")\n",
    "plt.plot([2, 3, 4, 5], ll_data3_GMM)\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: \n",
    "#### K_means clustering works best for the first data because the variance of all three clusters are the same and the mean of the data points in in the center of the clusters \n",
    "\n",
    "#### Based on the plots in K = 3, GMM performs better on data 2 since the variance of the three clusters is different so the spread of the data is different and K-means does not take the variance of the clusters into consideration\n",
    "\n",
    "#### Based on the NMI of K = 2 for the clustering of the data 3 using GMM or K-means, GMM performs better than K-means because the clusters in data 3 take on  Non-globular Shapes. The best clustering to use on the third data is DB_scan since the density of the clusters is what defines the shape and GMM depends on the data being from a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The End of the last HW in DM :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
