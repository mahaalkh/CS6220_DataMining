{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maha Ashraf Alkhairy \n",
    "## Data Mining HW 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Part: \n",
    "### Question 1.a: \n",
    "\n",
    "Items: a, b, c, d, e, f\n",
    "\n",
    "pass 1: set size 1: <br/>\n",
    "     C<sub>k</sub> = {a}:5, {b}:3, {c}:6, {d}:5, {e}:4, {f}:4 = L<sub>k</sub> (all are frequent) \n",
    "\n",
    "pass 2: set size 2:   \n",
    "     C<sub>k</sub> = {a, b}: 1 , {a, c}: 2 , {a, d}: 2, {a, e}: 2 , {a, f}: 1,\n",
    "                    {b, c}: 3, {b, d}: 1, {b, e}: 0, {b, f}: 2,\n",
    "                    {c, d}: 2, {c, e}: 3, {c, f}: 3, \n",
    "                    {d, e}: 1, {d, f}: 3, \n",
    "                    {e, f}: 1 <br/> L<sub>k</sub> = {b, c}: 3, {c, e}: 3, {c, f}: 3, {d, f}: 3\n",
    "     \n",
    "pass 3: set size 3: <br/>\n",
    "    C<sub>k</sub> = {{b, c, e}: 0, {c, e, f}: 1, {c, d, f}: 2, {b, c, f}: 2 <br/> L<sub>k</sub> = [] (empty no more frequent item sets)\n",
    "\n",
    "maximal frequent item sets: {a}, {b,c}, {c, e}, {c, f}, {d, f}\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1.b) \n",
    "     chose {c, f} from the maximal sets  and the association rule that satisfies the constraints (support/frequency at least 0.3 and confidence at least 0.6) is f -> c as shown in the table below: \n",
    "        \n",
    "| association rule       | support                      | confidence                         |\n",
    "|:----------------------:|:----------------------------:|:----------------------------------:|\n",
    "| c -> f                 | freq(c, f)/N = 3 / 10 = 0.3  | freq(c, f)/freq(c) = 3 / 6 = 0.5   |\n",
    "| f -> c                 | freq(c, f)/N = 3 / 10 = 0.3  | freq(c, f)/freq(f) = 3 / 4 = 0.75 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.a: \n",
    "--------------------------\n",
    "calculate frequency of items of size 1: \n",
    "    {a}:8 , {b}:6 , {c}:6 , {d}:4, {e}:4, {f}:1 --> remove f from the transactions since not frequent then draw the full FP-tree\n",
    "\n",
    "##### Full FP-tree: \n",
    "\n",
    "\n",
    "<img src=\"FP_full.jpg\" style=\"width: 500px;\"/> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.b: \n",
    "\n",
    "<img src=\"FP_d_subtree.jpg\" style=\"width: 300px;\"/> \n",
    "\n",
    "----------------------------------------------------------\n",
    "\n",
    "<img src=\"FP_tree_conditional.jpg\" style=\"width: 200px;\"/> \n",
    "\n",
    "----------------------------------------------------------\n",
    "\n",
    "<img src=\"FP_xd.jpg\" style=\"width: 300px;\"/> \n",
    "\n",
    "----------------------------------------------------------\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on the d's conditional FP-tree the frequent patterns are: \n",
    "  {d}, {b, d}, {c, d}, {a, b, d}, {a, c, d}, {a, b, c, d}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROGRAMMING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## imports for FP_Growth algorithm\n",
    "import pyspark \n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "# to parse and order outputs\n",
    "from operator import itemgetter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('publications.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_prev = sc.textFile('AP_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "\n",
    "The differences between the AP_train file and the Publications file syntactically is as follows: <br/>\n",
    "  1) The ordering of the lines are different: the publication index is placed between the venue and references. <br/>\n",
    "  2) There are empty references in publications.txt and not in AP_train.txt <br/>\n",
    "  3) There are no spaces after the special characters (ones that indecate whether it is a publication index, ....) in publication.txt whereas AP_train.txt has spaces <br/>\n",
    "  4) authors are , seperated in publications.txt and ; seperated in AP_train.txt\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "\n",
    "The differences in terms of statistics:  (publications.txt, AP_train.txt)\n",
    "\n",
    "| statistic              | publications.txt             | AP_train.txt                       |\n",
    "|:----------------------:|:----------------------------:|:----------------------------------:|\n",
    "| publications           |      2146341                 |    1976815                         |\n",
    "| authors                | 1232494                      | 1478733                            |\n",
    "| venues                 | 8707                         | 255685                             |\n",
    "| references             |   528263                     |     871089                         | \n",
    "| years                  |       80                     |             69                     |\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get the rdd for the specific items in AP_train.txt\n",
    "index_rdd_prev = rdd_prev.filter(\n",
    "                lambda l: re.match('^#index(.*)', l)).map(\n",
    "                lambda l: re.match('^#index(.*)', l).group(1).strip()).filter(\n",
    "                lambda l: l != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "venue_rdd_prev = rdd_prev.filter(\n",
    "                lambda l: re.match('^#c(.*)', l)).map(\n",
    "                lambda l: re.match('^#c(.*)', l).group(1).strip()).filter(\n",
    "                lambda l: l != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_rdd_prev = rdd_prev.filter(\n",
    "                lambda l: re.match('^#%(.*)', l)).map(\n",
    "                lambda l: re.match('^#%(.*)', l).group(1).strip()).filter(\n",
    "                lambda l: l != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yop_rdd_prev = rdd_prev.filter(\n",
    "                lambda l: re.match('^#t(.*)', l)).map(\n",
    "                lambda l: re.match('^#t(.*)', l).group(1).strip()).filter(\n",
    "                lambda l: l != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get the rdd for the specific things in publications.txt\n",
    "index_rdd = rdd.filter(\n",
    "                lambda l: re.match('^#index(.*)', l)).map(\n",
    "                lambda l: re.match('^#index(.*)', l).group(1).strip()).filter(\n",
    "                lambda l: l != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_rdd = rdd.filter(\n",
    "                lambda l: re.match('^#@(.*)', l)).map(\n",
    "                lambda l: re.match('^#@(.*)', l).group(1).strip()).filter(\n",
    "                lambda l: l != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "venue_rdd = rdd.filter(\n",
    "                lambda l: re.match('^#c(.*)', l)).map(\n",
    "                lambda l: re.match('^#c(.*)', l).group(1).strip()).filter(\n",
    "                lambda l: l != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_rdd = rdd.filter(\n",
    "                lambda l: re.match('^#%(.*)', l)).map(\n",
    "                lambda l: re.match('^#%(.*)', l).group(1).strip()).filter(\n",
    "                lambda l: l != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yop_rdd = rdd.filter(\n",
    "                lambda l: re.match('^#t(.*)', l)).map(\n",
    "                lambda l: re.match('^#t(.*)', l).group(1).strip()).filter(\n",
    "                lambda l: l != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## counting for publications.txt\n",
    "unique_index_pub_count = index_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_venue_count = venue_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_yop_count = yop_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_ref_count = ref_rdd.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique publications in publications.txt:  2146341\n",
      "number of unique venues in publications.txt:  8707\n",
      "number of unique years in publications.txt:  80\n",
      "number of unique references in publications.txt:  528263\n"
     ]
    }
   ],
   "source": [
    "print(\"number of unique publications in publications.txt: \", \n",
    "      unique_index_pub_count)\n",
    "## parsed later in the file when getting venues associated with authors\n",
    "# print(\"number of unique authors in publications.txt: \", unique_author_count)\n",
    "print(\"number of unique venues in publications.txt: \", \n",
    "      unique_venue_count)\n",
    "print(\"number of unique years in publications.txt: \", \n",
    "      unique_yop_count)\n",
    "print(\"number of unique references in publications.txt: \",\n",
    "      unique_ref_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## counting for AP_train.txt\n",
    "unique_index_pub_count_prev = index_rdd_prev.distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_venue_count_prev = venue_rdd_prev.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_yop_count_prev = yop_rdd_prev.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_ref_count_prev = ref_rdd_prev.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique publications in AP_train.txt:  1976815\n",
      "number of unique venues in AP_train.txt:  255685\n",
      "number of unique years in AP_train.txt:  69\n",
      "number of unique references in AP_train.txt:  871089\n"
     ]
    }
   ],
   "source": [
    "print(\"number of unique publications in AP_train.txt: \", unique_index_pub_count_prev)\n",
    "# print(\"number of unique authors in AP_train.txt: \", unique_author_count_prev)\n",
    "print(\"number of unique venues in AP_train.txt: \", unique_venue_count_prev)\n",
    "print(\"number of unique years in AP_train.txt: \", unique_yop_count_prev)\n",
    "print(\"number of unique references in AP_train.txt: \", unique_ref_count_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions = author_rdd.map(lambda l: list(set(l.strip().split(','))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## support threshold 1e-4\n",
    "model_1 = FPGrowth.train(transactions, minSupport=1e-4,  numPartitions=1)\n",
    "result_1 = model_1.freqItemsets().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['Christos H. Papadimitriou'], freq=355)\n",
      "FreqItemset(items=['Fumiyuki Adachi'], freq=227)\n",
      "FreqItemset(items=['Hao Wang'], freq=348)\n",
      "FreqItemset(items=['Christian S. Jensen'], freq=302)\n",
      "FreqItemset(items=['Bernd Freisleben'], freq=226)\n",
      "FreqItemset(items=['Kishor S. Trivedi'], freq=298)\n",
      "FreqItemset(items=['Mubarak Shah'], freq=225)\n",
      "FreqItemset(items=['David Zhang'], freq=390)\n",
      "FreqItemset(items=['Hui Liu'], freq=248)\n",
      "FreqItemset(items=['Lei Chen'], freq=380)\n"
     ]
    }
   ],
   "source": [
    "for i in result_1[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## support threshold 1e-5\n",
    "model_2 = FPGrowth.train(transactions, minSupport=1e-5,  numPartitions=1)\n",
    "result_2 = model_2.freqItemsets().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['Philip Levis'], freq=58)\n",
      "FreqItemset(items=['Myung-Sun Baek'], freq=24)\n",
      "FreqItemset(items=['Mark J. Karol'], freq=26)\n",
      "FreqItemset(items=['Xiang Ji'], freq=23)\n",
      "FreqItemset(items=['Ork de Rooij'], freq=22)\n",
      "FreqItemset(items=['Gaoxi Xiao'], freq=32)\n",
      "FreqItemset(items=['Leslie G. Valiant'], freq=109)\n",
      "FreqItemset(items=['Martin Margala'], freq=81)\n",
      "FreqItemset(items=['Jason Dykes'], freq=30)\n",
      "FreqItemset(items=['Satya Dharanipragada'], freq=23)\n"
     ]
    }
   ],
   "source": [
    "for i in result_2[:10]: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## support threshold 0.5e-5\n",
    "model_3 = FPGrowth.train(transactions, minSupport=0.5e-5,  numPartitions=1)\n",
    "result_3 = model_3.freqItemsets().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['Wei Wang'], freq=1293)\n",
      "FreqItemset(items=['Wei Zhang'], freq=856)\n",
      "FreqItemset(items=['Lei Zhang'], freq=841)\n",
      "FreqItemset(items=['Wei Li'], freq=805)\n",
      "FreqItemset(items=['H. Vincent Poor'], freq=735)\n",
      "FreqItemset(items=['Jun Wang'], freq=717)\n",
      "FreqItemset(items=['Philip S. Yu'], freq=711)\n",
      "FreqItemset(items=['Wen Gao'], freq=707)\n",
      "FreqItemset(items=['Thomas S. Huang'], freq=691)\n",
      "FreqItemset(items=['Lei Wang'], freq=690)\n"
     ]
    }
   ],
   "source": [
    "for i in result_3[:10]: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## support threshold 1e-6 , does not work with large data (runs out of memory)\n",
    "# model_4 = FPGrowth.train(transactions, minSupport=1e-6,  numPartitions=1)\n",
    "# result_4 = model_5.freqItemsets().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2a: \n",
    "When we succesively decrease the support threshold, the number of frequent items and the co-occurence of the authors increases to the point of not being able to handle the calculation with the heap memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of frequent items when the support threshold is 1e-4:  665\n",
      "number of frequent items when the support threshold is 1e-5:  58955\n",
      "number of frequent items when the support threshold is 0.5e-5:  168269\n"
     ]
    }
   ],
   "source": [
    "## for the authors\n",
    "print(\"number of frequent items when the support threshold is 1e-4: \", len(result_1))\n",
    "print(\"number of frequent items when the support threshold is 1e-5: \", len(result_2))\n",
    "print(\"number of frequent items when the support threshold is 0.5e-5: \", len(result_3))\n",
    "# print(\"number of frequent items when the support threshold is 1e-6: \", len(result_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coocurrences(frequent_items, item, num_top): \n",
    "    \"\"\"\n",
    "    gets the items that frequently coocur with the given item\n",
    "    :frequent_items: [[string, ...], ...]\n",
    "    :item: string\n",
    "    :num_top: int\n",
    "    \"\"\"\n",
    "    lst = filter(lambda r: item in r[0], frequent_items)\n",
    "    top_x = []\n",
    "    for l in lst:\n",
    "        for i in l[0]: \n",
    "            if len(top_x) == num_top: \n",
    "                break\n",
    "            if i == item: \n",
    "                continue \n",
    "            if i in top_x: \n",
    "                continue\n",
    "            else: \n",
    "                top_x.append(i)\n",
    "    return top_x\n",
    "    \n",
    "def print_cooccurences(colist): \n",
    "    \"\"\"\n",
    "    print the items in the list seperated by a new line \n",
    "    \"\"\"\n",
    "    for i in colist: \n",
    "        print(\"\\t\", i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2b: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 coauthors of Rakesh Agrawal ordered by frequency: \n",
      "\t Ramakrishnan Srikant\n",
      "\t Jerry Kiernan\n",
      "\t H. V. Jagadish\n",
      "\t Yirong Xu\n",
      "\t Michael J. Carey\n",
      "\n",
      " -----------\n",
      "The top 5 coauthors of Jiawei Han ordered by frequency: \n",
      "\t Xifeng Yan\n",
      "\t Philip S. Yu\n",
      "\t Jian Pei\n",
      "\t Yizhou Sun\n",
      "\t Xin Jin\n",
      "\n",
      " -----------\n",
      "The top 5 coauthors of Zoubin Ghahramani ordered by frequency: \n",
      "\t David L. Wild\n",
      "\t Katherine A. Heller\n",
      "\t Michael I. Jordan\n",
      "\n",
      " -----------\n",
      "The top 5 coauthors of  Christos Faloutsos ordered by frequency: \n",
      "\t Hanghang Tong\n",
      "\t Spiros Papadimitriou\n",
      "\t Jimeng Sun\n",
      "\t Caetano Traina Jr.\n",
      "\t Agma J. M. Traina\n",
      "\n",
      " -----------\n"
     ]
    }
   ],
   "source": [
    "# when the support threshold is 0.5e-5\n",
    "author = ['Rakesh Agrawal', 'Jiawei Han', 'Zoubin Ghahramani', 'Christos Faloutsos']\n",
    "result_3.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "coauthors_1 = get_coocurrences(result_3, author[0], 5)\n",
    "coauthors_2 = get_coocurrences(result_3, author[1], 5)\n",
    "coauthors_3 = get_coocurrences(result_3, author[2], 5)\n",
    "coauthors_4 = get_coocurrences(result_3, author[3], 5)\n",
    "\n",
    "\n",
    "print(\"The top 5 coauthors of Rakesh Agrawal ordered by frequency: \") \n",
    "print_cooccurences(coauthors_1)\n",
    "print(\"\\n -----------\") \n",
    "print(\"The top 5 coauthors of Jiawei Han ordered by frequency: \") \n",
    "print_cooccurences(coauthors_2)\n",
    "print(\"\\n -----------\")\n",
    "print(\"The top 5 coauthors of Zoubin Ghahramani ordered by frequency: \")\n",
    "print_cooccurences(coauthors_3)\n",
    "print(\"\\n -----------\") \n",
    "print(\"The top 5 coauthors of  Christos Faloutsos ordered by frequency: \")\n",
    "print_cooccurences(coauthors_4)\n",
    "print(\"\\n -----------\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transactions_by_author = sc.textFile(\"venue_baskets.txt\").map(lambda l: list(set(l.strip().split(' ; ')))).filter(lambda s: s != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_ven_1 = FPGrowth.train(transactions_by_author, minSupport=1e-3,  numPartitions=1)\n",
    "result_ven_1 = model_ven_1.freqItemsets().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_ven_2 = FPGrowth.train(transactions_by_author, minSupport=0.4e-3,  numPartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_ven_2 = model_ven_2.freqItemsets().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_ven_3 = FPGrowth.train(transactions_by_author, minSupport=1e-4,  numPartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# does not work on large data sets (memory error)\n",
    "# result_ven_3 = model_ven_3.freqItemsets().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['ICSLP'], freq=4022)\n",
      "FreqItemset(items=['ICSLP', 'EUROSPEECH'], freq=1660)\n",
      "FreqItemset(items=['ICSLP', 'INTERSPEECH'], freq=1366)\n",
      "FreqItemset(items=['Ad Hoc Networks'], freq=1632)\n",
      "FreqItemset(items=['Journal of Chemical Information and Computer Sciences'], freq=3871)\n",
      "FreqItemset(items=['Ann. Math. Artif. Intell.'], freq=1588)\n",
      "FreqItemset(items=['SIAM J. Comput.'], freq=3000)\n",
      "FreqItemset(items=['SIAM J. Comput.', 'Inf. Process. Lett.'], freq=1249)\n",
      "FreqItemset(items=['SIAM J. Comput.', 'Theor. Comput. Sci.'], freq=1349)\n",
      "FreqItemset(items=['SIAM J. Comput.', 'CoRR'], freq=1455)\n"
     ]
    }
   ],
   "source": [
    "for i in result_ven_1[:10]: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['ICSLP'], freq=4022)\n",
      "FreqItemset(items=['ICSLP', 'EUROSPEECH'], freq=1660)\n",
      "FreqItemset(items=['ICSLP', 'EUROSPEECH', 'INTERSPEECH'], freq=929)\n",
      "FreqItemset(items=['ICSLP', 'INTERSPEECH'], freq=1366)\n",
      "FreqItemset(items=['ICSLP', 'ICASSP'], freq=504)\n",
      "FreqItemset(items=['J. Computer-Mediated Communication'], freq=1070)\n",
      "FreqItemset(items=['IEEE Communications Surveys and Tutorials'], freq=797)\n",
      "FreqItemset(items=['Ad Hoc Networks'], freq=1632)\n",
      "FreqItemset(items=['Ad Hoc Networks', 'GLOBECOM'], freq=585)\n",
      "FreqItemset(items=['Ad Hoc Networks', 'ICC'], freq=539)\n"
     ]
    }
   ],
   "source": [
    "for i in result_ven_2[:10]: \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in result_ven_3[:10]: \n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3a\n",
    "When we decreas the support threshold the number of frequent items inceases and that co-occurence of venues increases sometime to the point where the heap memory cannot handle it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of frequent item sets with 1e-3:  981\n",
      "number of frequent item sets with 0.4e-3:  4359\n"
     ]
    }
   ],
   "source": [
    "print(\"number of frequent item sets with 1e-3: \", len(result_ven_1))\n",
    "print(\"number of frequent item sets with 0.4e-3: \", len(result_ven_2))\n",
    "# print(\"number of frequent item sets with 1e-4: \", len(result_ven_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for Machine learning: the top ten publications that authors also publish in are: \n",
      "\t CoRR\n",
      "\t ICML\n",
      "\t Neural Computation\n",
      "\t Journal of Machine Learning Research - Proceedings Track\n",
      "\t Journal of Machine Learning Research\n",
      "\t IEEE Trans. Pattern Anal. Mach. Intell.\n",
      "\t CVPR\n",
      "\t Neurocomputing\n",
      "\t Neural Networks\n",
      "\t IJCAI\n",
      "\n",
      "--------------\n",
      "for data mining: the top ten publications that authors also publish in are: \n",
      "\t CoRR\n",
      "\t ICDM\n",
      "\t CIKM\n",
      "\t IEEE Trans. Knowl. Data Eng.\n",
      "\t SDM\n",
      "\t ICML\n",
      "\t WWW\n",
      "\n",
      "--------------\n",
      "for data bases: the top ten publications that authors also publish in are: \n",
      "\t ICDE\n",
      "\t SIGMOD Conference\n",
      "\t CoRR\n",
      "\t IEEE Trans. Knowl. Data Eng.\n",
      "\t SIGMOD Record\n",
      "\t EDBT\n",
      "\t CIKM\n",
      "\t IEEE Data Eng. Bull.\n",
      "\t VLDB J.\n",
      "\t ACM Trans. Database Syst.\n",
      "\n",
      "--------------\n",
      "for computer networks: the top ten publications that authors also publish in are: \n",
      "\t GLOBECOM\n",
      "\t ICC\n",
      "\t CoRR\n",
      "\t IEEE/ACM Trans. Netw.\n",
      "\t IEEE Journal on Selected Areas in Communications\n",
      "\t Computer Networks\n",
      "\t Computer Communications\n",
      "\t ICDCS\n",
      "\t IEEE Trans. Parallel Distrib. Syst.\n",
      "\t WCNC\n",
      "\n",
      "--------------\n",
      "for natural language processing: the top ten publications that authors also publish in are: \n",
      "\t COLING\n",
      "\t LREC\n",
      "\t CoRR\n",
      "\t EMNLP\n",
      "\t HLT-NAACL\n",
      "\t INTERSPEECH\n",
      "\t Computational Linguistics\n",
      "\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "ml = \"NIPS\"\n",
    "dm = \"KDD\"\n",
    "db = \"VLDB\"\n",
    "cn = \"INFOCOM\"\n",
    "nlp = \"ACL\"\n",
    "\n",
    "result_ven_2.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "top_venues_ml = get_coocurrences(result_ven_2, ml, 10)\n",
    "top_venues_dm = get_coocurrences(result_ven_2, dm, 10)\n",
    "top_venues_db = get_coocurrences(result_ven_2, db, 10)\n",
    "top_venues_cn = get_coocurrences(result_ven_2, cn, 10)\n",
    "top_venues_nlp = get_coocurrences(result_ven_2, nlp, 10)\n",
    "\n",
    "\n",
    "# venue_groups_ml = filter(lambda r: ml in r[0], result_ven_2) \n",
    "# venue_groups_dm = filter(lambda r: dm in r[0], result_ven_2) \n",
    "# venue_groups_db = filter(lambda r: db in r[0], result_ven_2) \n",
    "# venue_groups_cn = filter(lambda r: cn in r[0], result_ven_2) \n",
    "# venue_groups_nlp = filter(lambda r: nlp in r[0], result_ven_2) \n",
    "\n",
    "\n",
    "# def get_the_top_10_associated_ven(lst, ven): \n",
    "#     \"\"\"\n",
    "#     :lst: [['string', ...], ...] \n",
    "#     \"\"\"\n",
    "#     top = []\n",
    "#     for l in lst:\n",
    "#         for v in l[0]: \n",
    "#             if len(top) == 10: \n",
    "#                 break\n",
    "#             if v == ven: \n",
    "#                 continue \n",
    "#             if v in top: \n",
    "#                 continue\n",
    "#             else: \n",
    "#                 top.append(v)\n",
    "#     return top\n",
    "\n",
    "\n",
    "print(\"for Machine learning: the top ten publications that authors also publish in are: \") \n",
    "print_cooccurences(top_venues_ml)\n",
    "print(\"\\n--------------\")\n",
    "\n",
    "print(\"for data mining: the top ten publications that authors also publish in are: \") \n",
    "print_cooccurences(top_venues_dm)\n",
    "print(\"\\n--------------\")\n",
    "\n",
    "print(\"for data bases: the top ten publications that authors also publish in are: \")\n",
    "print_cooccurences(top_venues_db)\n",
    "print(\"\\n--------------\")\n",
    "      \n",
    "print(\"for computer networks: the top ten publications that authors also publish in are: \")\n",
    "print_cooccurences(top_venues_cn)\n",
    "print(\"\\n--------------\")\n",
    "      \n",
    "print(\"for natural language processing: the top ten publications that authors also publish in are: \") \n",
    "print_cooccurences(top_venues_nlp)\n",
    "print(\"\\n--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
